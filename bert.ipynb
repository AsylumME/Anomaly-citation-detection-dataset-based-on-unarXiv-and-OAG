{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Bert获得文章向量***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "from cogdl.oag import oagbert\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# 检查CUDA是否可用，并相应地设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载OAG-BERT模型，并将其移至指定设备\n",
    "tokenizer, model = oagbert(\"oagbert-v2\")\n",
    "model.to(device)\n",
    "\n",
    "# 定义一个函数来处理每篇论文\n",
    "def process_paper(paper, model, device):\n",
    "    title = paper.get(\"Title\", \"\")\n",
    "    abstract = paper.get(\"Abstract\", \"\")\n",
    "    authors = paper.get(\"Authors\", [])\n",
    "    venue = paper.get(\"Journal\", \"\")\n",
    "    affiliations = paper.get(\"Affiliations\", [])\n",
    "    concepts = []  # 没有概念字段，所以使用空列表\n",
    "\n",
    "    # 构建模型输入\n",
    "    input_ids, input_masks, token_type_ids, masked_lm_labels, position_ids, position_ids_second, masked_positions, num_spans = model.build_inputs(\n",
    "        title=title, abstract=abstract, venue=venue, authors=authors, concepts=concepts, affiliations=affiliations\n",
    "    )\n",
    "\n",
    "    # 将输入数据移至设备\n",
    "    input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.LongTensor(input_masks).unsqueeze(0).to(device)\n",
    "    position_ids = torch.LongTensor(position_ids).unsqueeze(0).to(device)\n",
    "    position_ids_second = torch.LongTensor(position_ids_second).unsqueeze(0).to(device)\n",
    "\n",
    "    # 运行模型\n",
    "    with torch.no_grad():\n",
    "        sequence_output, pooled_output = model.bert.forward(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_all_encoded_layers=False,\n",
    "            checkpoint_activations=False,\n",
    "            position_ids=position_ids,\n",
    "            position_ids_second=position_ids_second\n",
    "        )\n",
    "\n",
    "    # 将输出移回CPU，并转换为numpy数组\n",
    "    return pooled_output.cpu().detach().numpy()\n",
    "\n",
    "# 读取目标索引\n",
    "with open(\"network_graph.json\", \"r\") as f:\n",
    "    network_graph = json.load(f)\n",
    "    target_indices = list(network_graph.keys())  # 获取所有节点的索引值\n",
    "\n",
    "\n",
    "# 打开输出文件\n",
    "with open(\"output_vectors.json\", \"w\") as file:\n",
    "    # 使用ijson逐步读取并处理文件\n",
    "    with open(\"Aminer-Paper.json\", \"r\", encoding=\"utf-8\") as infile:\n",
    "        papers = ijson.items(infile, 'item')\n",
    "\n",
    "        # 使用tqdm显示进度\n",
    "        for paper in tqdm(papers, total=2092356, desc=\"Processing Papers\"):\n",
    "            index = paper.get(\"Index\")\n",
    "            if str(index) in target_indices:\n",
    "                output = process_paper(paper, model, device)\n",
    "                # 将每个输出向量写入文件，每个向量一行\n",
    "                file.write(json.dumps({\"Index\": index, \"Vector\": output.tolist()}) + \"\\n\")\n",
    "\n",
    "print(\"处理完成，输出向量已保存至output_vectors.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***筛选数据集***\n",
    "\n",
    "***首先先要找出到底有哪些文章，在对文章对进行随机处理，然后求出随即处理对中的LOF Cos LDA，以及对应的flag = 1(异常)***\n",
    "\n",
    "***再挑选非常适合的正常的数据对作为正常 flag= 0***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def load_network_graph(file_name):\n",
    "    with open(file_name, \"r\") as file:\n",
    "        network_graph = json.load(file)\n",
    "    return network_graph\n",
    "\n",
    "def get_unique_article_ids(network_graph):\n",
    "    article_ids = set()\n",
    "    for article_id, links in network_graph.items():\n",
    "        article_ids.add(int(article_id))  \n",
    "        article_ids.update(map(int, links['in']))  \n",
    "        article_ids.update(map(int, links['out']))  \n",
    "    return list(article_ids)\n",
    "\n",
    "def generate_random_citations(article_ids, network_graph, count=2000):\n",
    "    generated_pairs = set()\n",
    "    existing_pairs = set()\n",
    "\n",
    "    for article_id, links in network_graph.items():\n",
    "        for cited in links['in']:\n",
    "            existing_pairs.add((int(article_id), int(cited)))\n",
    "\n",
    "    while len(generated_pairs) < count:\n",
    "        cite = random.choice(article_ids)\n",
    "        cited = random.choice(article_ids)\n",
    "\n",
    "        if cite != cited and (cite, cited) not in existing_pairs and (cite, cited) not in generated_pairs:\n",
    "            generated_pairs.add((cite, cited))\n",
    "\n",
    "    return list(generated_pairs)\n",
    "\n",
    "network_graph_file = 'network_graph.json'\n",
    "network_graph = load_network_graph(network_graph_file)\n",
    "article_ids = get_unique_article_ids(network_graph)\n",
    "random_citations = generate_random_citations(article_ids, network_graph)\n",
    "\n",
    "json_structure = [{\"Cite\": pair[0], \"Cited\": pair[1],\"Flag\":1,} for pair in random_citations]\n",
    "with open(\"random_citations.json\", \"w\") as json_file:\n",
    "    json.dump(json_structure, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_network_graphs(network_graph_file, random_citations_file, output_file):\n",
    "    with open(network_graph_file, 'r') as file:\n",
    "        network_graph = json.load(file)\n",
    "\n",
    "    with open(random_citations_file, 'r') as file:\n",
    "        random_citations = json.load(file)\n",
    "\n",
    "    max_index = max(item[\"index\"] for item in network_graph)\n",
    "\n",
    "    for i, citation in enumerate(random_citations, start=max_index + 1):\n",
    "        citation_entry = {\n",
    "            \"index\": i,\n",
    "            \"Cite\": citation[\"Cite\"],\n",
    "            \"Cited\": citation[\"Cited\"],\n",
    "            \"flag\": citation.get(\"Flag\", 1)  \n",
    "        }\n",
    "        network_graph.append(citation_entry)\n",
    "\n",
    "    with open(output_file, 'w') as file:\n",
    "        json.dump(network_graph, file, indent=4)\n",
    "\n",
    "network_graph_file = 'converted_network_graph.json'\n",
    "random_citations_file = 'random_citations.json'\n",
    "output_file = 'merged_network_graph.json'\n",
    "\n",
    "merge_network_graphs(network_graph_file, random_citations_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LOF算法***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# 从增量数据中整合所有独特的文章ID\n",
    "def get_unique_article_ids(increment_data):  #假设这里的increment_data是已经把增加的和原本的数据对都已经整合到一起了！！！！\n",
    "    article_ids = set()\n",
    "    for item in increment_data:\n",
    "        article_ids.add(item[\"Cite\"])\n",
    "        article_ids.add(item[\"Cited\"])\n",
    "    return list(article_ids)\n",
    "\n",
    "# 从output_vectors.json文件中提取相应的文章向量\n",
    "def get_article_vectors(article_ids, vector_file):\n",
    "    vectors = {}\n",
    "    with open(vector_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            article_id = data[\"Index\"]\n",
    "            if article_id in article_ids:\n",
    "                vectors[article_id] = data[\"Vector\"][0]  # 仅使用第一个向量\n",
    "    return vectors\n",
    "\n",
    "def detect_anomalies_with_lof(vectors, n_neighbors=20, contamination='auto'):\n",
    "    \"\"\"\n",
    "    使用LOF算法检测异常值。\n",
    "    \n",
    "    参数:\n",
    "    data (array-like): 需要检测的数据点，应为二维数组形式。\n",
    "    n_neighbors (int): 用于计算LOF的邻居数,默认为20。\n",
    "    contamination (float or 'auto'): 数据中预期的异常值比例。\n",
    "\n",
    "    返回:\n",
    "    is_anomaly (numpy array): 数据点的异常标记,1表示正常点,-1表示异常点。\n",
    "    scores (numpy array): 数据点的异常分数，分数越低表示越可能是异常值。\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.array(list(vectors.values()))\n",
    "    # 初始化LOF检测器\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination, novelty=False)\n",
    "    # 训练模型并预测\n",
    "    is_anomaly = lof.fit_predict(data)\n",
    "\n",
    "    # 获取每个点的异常分数\n",
    "    scores = -lof.negative_outlier_factor_  # 转换为正数，数值越大表示越可能是异常\n",
    "\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # 假设 scores 是通过 LOF 算法计算得到的异常分数数组\n",
    "    scores = -lof.negative_outlier_factor_\n",
    "\n",
    "    # 确定一个阈值，这里假设使用分数的平均值加上两倍的标准差作为阈值\n",
    "    threshold = np.mean(scores) + 2 * np.std(scores)\n",
    "\n",
    "    # 初始化一个数组来存储异常标记\n",
    "    is_anomaly = np.ones_like(scores, dtype=int)  # 默认所有点为正常\n",
    "\n",
    "    # 标记异常值\n",
    "    is_anomaly[scores > threshold] = -1  # 分数高于阈值的点被标记为异常\n",
    "\n",
    "    # 现在 is_anomaly 数组包含了基于自定义阈值的异常标记，-1 表示异常，1 表示正常 \n",
    "    \"\"\"\n",
    "    \n",
    "    return is_anomaly, scores\n",
    "\n",
    "def map_scores_to_citations(increment_data, vectors, scores):\n",
    "    scores_dict = {article_id: score for article_id, score in zip(vectors.keys(), scores)}\n",
    "    results = []\n",
    "\n",
    "    # 遍历增量数据\n",
    "    for item in increment_data:\n",
    "        cite_id = item[\"Cite\"]\n",
    "        cited_id = item[\"Cited\"]\n",
    "\n",
    "        cite_score = scores_dict.get(cite_id, 0)\n",
    "        cited_score = scores_dict.get(cited_id, 0)\n",
    "\n",
    "        # 计算综合分数，这里我们简单地取两者分数的平均值，也可以根据需要采用不同的策略\n",
    "        combined_score = (cite_score + cited_score) / 2\n",
    "        # 这个方法有待讨论\n",
    "\n",
    "        results.append({\n",
    "            \"Cite\": cite_id,\n",
    "            \"Cited\": cited_id,\n",
    "            \"LOF_Score\": combined_score\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ########################################################################################################################################\n",
    "# LOF算法总的输入是output_vector.json  converted_network_graph.json  上次迭代结果剩下的result  flag = 0中结果比较好的引用对(将他转变为result格式)\n",
    "# LOF算法的输出是result的结果（这个结果不仅可以在下一次增量学习迭代的时候拿来利用，同时也可以用作接下来分数的整合过程中）\n",
    "# #########################################################################################################################################\n",
    "\n",
    "# 加载增量数据\n",
    "with open(\"merged_network_graph.json\", \"r\") as f:\n",
    "    increment_data = json.load(f)\n",
    "\n",
    "article_ids = get_unique_article_ids(increment_data)\n",
    "vectors = get_article_vectors(article_ids, \"output_vectors.json\")  # 确保文件名正确\n",
    "is_anomaly, scores = detect_anomalies_with_lof(vectors)\n",
    "results = map_scores_to_citations(increment_data, vectors, scores)\n",
    "with open(\"LOF.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LDA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luzixuan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Preprocessing documents: 100%|██████████| 11999/11999 [00:22<00:00, 540.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: (0, '0.122*\"busi\" + 0.089*\"process\" + 0.063*\"e\" + 0.036*\"event\" + 0.036*\"tempor\" + 0.033*\"spatial\" + 0.027*\"g\" + 0.018*\"time\" + 0.015*\"geograph\" + 0.012*\"organis\"')\n",
      "Topic 2: (1, '0.089*\"databas\" + 0.085*\"data\" + 0.067*\"system\" + 0.037*\"manag\" + 0.030*\"workflow\" + 0.023*\"secur\" + 0.021*\"applic\" + 0.019*\"transact\" + 0.016*\"mine\" + 0.015*\"design\"')\n",
      "Topic 3: (2, '0.035*\"book\" + 0.023*\"learn\" + 0.018*\"use\" + 0.015*\"chapter\" + 0.014*\"student\" + 0.014*\"2\" + 0.014*\"includ\" + 0.013*\"1\" + 0.012*\"publish\" + 0.011*\"design\"')\n",
      "Topic 4: (3, '0.259*\"web\" + 0.057*\"applic\" + 0.044*\"site\" + 0.035*\"page\" + 0.034*\"warehous\" + 0.029*\"user\" + 0.020*\"navig\" + 0.017*\"internet\" + 0.016*\"hypertext\" + 0.016*\"link\"')\n",
      "Topic 5: (4, '0.112*\"test\" + 0.081*\"graph\" + 0.056*\"visual\" + 0.024*\"gener\" + 0.022*\"imag\" + 0.018*\"graphic\" + 0.016*\"diagram\" + 0.015*\"label\" + 0.014*\"structur\" + 0.014*\"node\"')\n",
      "Topic 6: (5, '0.055*\"algorithm\" + 0.041*\"time\" + 0.036*\"problem\" + 0.020*\"state\" + 0.018*\"comput\" + 0.015*\"optim\" + 0.015*\"verif\" + 0.012*\"check\" + 0.011*\"machin\" + 0.011*\"method\"')\n",
      "Topic 7: (6, '0.226*\"object\" + 0.107*\"orient\" + 0.051*\"class\" + 0.024*\"type\" + 0.015*\"inherit\" + 0.013*\"structur\" + 0.013*\"dynam\" + 0.012*\"oo\" + 0.011*\"method\" + 0.010*\"relationship\"')\n",
      "Topic 8: (7, '0.017*\"one\" + 0.013*\"mani\" + 0.011*\"work\" + 0.010*\"way\" + 0.009*\"question\" + 0.008*\"may\" + 0.007*\"even\" + 0.007*\"like\" + 0.007*\"would\" + 0.006*\"idea\"')\n",
      "Topic 9: (8, '0.096*\"program\" + 0.058*\"languag\" + 0.030*\"code\" + 0.019*\"use\" + 0.018*\"java\" + 0.016*\"implement\" + 0.013*\"tool\" + 0.011*\"gener\" + 0.011*\"programm\" + 0.010*\"execut\"')\n",
      "Topic 10: (9, '0.038*\"develop\" + 0.037*\"softwar\" + 0.028*\"process\" + 0.025*\"manag\" + 0.024*\"project\" + 0.020*\"product\" + 0.017*\"system\" + 0.016*\"engin\" + 0.015*\"inform\" + 0.015*\"knowledg\"')\n",
      "Topic 11: (10, '0.036*\"system\" + 0.030*\"servic\" + 0.023*\"applic\" + 0.021*\"distribut\" + 0.018*\"architectur\" + 0.018*\"compon\" + 0.016*\"network\" + 0.012*\"provid\" + 0.012*\"base\" + 0.012*\"environ\"')\n",
      "Topic 12: (11, '0.108*\"role\" + 0.061*\"net\" + 0.041*\"play\" + 0.036*\"control\" + 0.031*\"access\" + 0.029*\"petri\" + 0.017*\"biolog\" + 0.015*\"author\" + 0.011*\"microsoft\" + 0.011*\"administr\"')\n",
      "Topic 13: (12, '0.038*\"system\" + 0.037*\"user\" + 0.028*\"interfac\" + 0.017*\"interact\" + 0.016*\"use\" + 0.016*\"specif\" + 0.010*\"modul\" + 0.009*\"implement\" + 0.008*\"behavior\" + 0.008*\"describ\"')\n",
      "Topic 14: (13, '0.064*\"relat\" + 0.041*\"databas\" + 0.034*\"entiti\" + 0.033*\"depend\" + 0.031*\"schema\" + 0.030*\"relationship\" + 0.025*\"attribut\" + 0.018*\"normal\" + 0.017*\"function\" + 0.015*\"form\"')\n",
      "Topic 15: (14, '0.037*\"research\" + 0.014*\"inform\" + 0.014*\"paper\" + 0.013*\"system\" + 0.013*\"studi\" + 0.012*\"agent\" + 0.011*\"comput\" + 0.011*\"problem\" + 0.010*\"work\" + 0.010*\"discuss\"')\n",
      "Topic 16: (15, '0.084*\"data\" + 0.049*\"queri\" + 0.025*\"xml\" + 0.024*\"databas\" + 0.023*\"inform\" + 0.020*\"schema\" + 0.016*\"structur\" + 0.016*\"document\" + 0.013*\"base\" + 0.013*\"view\"')\n",
      "Topic 17: (16, '0.089*\"model\" + 0.023*\"use\" + 0.021*\"system\" + 0.017*\"approach\" + 0.017*\"base\" + 0.015*\"paper\" + 0.015*\"requir\" + 0.013*\"specif\" + 0.013*\"domain\" + 0.012*\"conceptu\"')\n",
      "Topic 18: (17, '0.068*\"design\" + 0.055*\"softwar\" + 0.043*\"uml\" + 0.028*\"use\" + 0.026*\"architectur\" + 0.026*\"develop\" + 0.024*\"diagram\" + 0.021*\"model\" + 0.021*\"pattern\" + 0.020*\"system\"')\n",
      "Topic 19: (18, '0.025*\"constraint\" + 0.024*\"logic\" + 0.021*\"semant\" + 0.017*\"rule\" + 0.017*\"express\" + 0.016*\"languag\" + 0.014*\"formal\" + 0.011*\"reason\" + 0.011*\"set\" + 0.010*\"properti\"')\n",
      "Topic 20: (19, '0.028*\"use\" + 0.027*\"perform\" + 0.020*\"result\" + 0.016*\"evalu\" + 0.016*\"method\" + 0.014*\"techniqu\" + 0.013*\"measur\" + 0.012*\"data\" + 0.012*\"analysi\" + 0.011*\"studi\"')\n",
      "LDA模型的主题已保存到文件: lda_topics.txt\n",
      "每篇文章的主题分布向量及其索引已保存到文件: document_topics_vectors.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# Ensure the necessary NLTK downloads\n",
    "nltk.download('stopwords')\n",
    "\n",
    "num_topics = 20\n",
    "passes = 30\n",
    "\n",
    "# 加载JSON文件并提取摘要\n",
    "input_filename = './abstracts.json'\n",
    "with open(input_filename, 'r', encoding='utf-8') as file:\n",
    "    # 加载数据\n",
    "    data = json.load(file)\n",
    "    # 仅保留摘要非空的项\n",
    "    filtered_data = [item for item in data if item['Abstract']]\n",
    "    \n",
    "# 从过滤后的数据中提取摘要\n",
    "documents = [item['Abstract'] for item in filtered_data]\n",
    "\n",
    "# 文档预处理函数\n",
    "def preprocess(documents):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocessed_texts = []\n",
    "    for doc in tqdm(documents, desc=\"Preprocessing documents\"):\n",
    "        raw = doc.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        stopped_tokens = [i for i in tokens if i not in stop_words]\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in stopped_tokens]\n",
    "        preprocessed_texts.append(stemmed_tokens)\n",
    "    return preprocessed_texts\n",
    "\n",
    "# 预处理摘要\n",
    "preprocessed_texts = preprocess(documents)\n",
    "\n",
    "# 创建字典和语料库\n",
    "dictionary = corpora.Dictionary(preprocessed_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "# 构建LDA模型\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes, update_every=1, eval_every=None)\n",
    "\n",
    "# 打印并保存主题\n",
    "output_filename = 'lda_topics.txt'\n",
    "with open(output_filename, 'w', encoding='utf-8') as out_file:\n",
    "    for i, topic in enumerate(lda_model.print_topics(num_words=10)):\n",
    "        print(f\"Topic {i+1}: {topic}\")\n",
    "        out_file.write(f\"Topic {i+1}: {topic}\\n\")\n",
    "\n",
    "print(f'LDA模型的主题已保存到文件: {output_filename}')\n",
    "\n",
    "# 保存文档主题向量为JSON\n",
    "def save_document_topics_as_vectors_json(data, corpus, lda_model, num_topics, output_filename):\n",
    "    doc_topics_vectors = []\n",
    "    for item, doc_bow in zip(data, corpus):\n",
    "        topic_vector = [0.0] * num_topics\n",
    "        topics = lda_model.get_document_topics(doc_bow, minimum_probability=0.0)\n",
    "        for topic_id, prob in topics:\n",
    "            topic_vector[topic_id] = float(prob)\n",
    "        doc_index = item['Index']\n",
    "        doc_topics_vectors.append({\"Index\": doc_index, \"Vector\": topic_vector})\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as out_file:\n",
    "        json.dump(doc_topics_vectors, out_file, indent=4)\n",
    "\n",
    "doc_topics_vectors_filename = 'document_topics_vectors.json'\n",
    "save_document_topics_as_vectors_json(filtered_data, corpus, lda_model, num_topics, doc_topics_vectors_filename)\n",
    "\n",
    "print(f'每篇文章的主题分布向量及其索引已保存到文件: {doc_topics_vectors_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def infer_topic_vector_for_article(article_index, network_graph, lda_vectors):\n",
    "    # 获取当前文章的引用和被引用文章索引列表\n",
    "    neighbors = network_graph.get(str(article_index), {}).get('in', []) + network_graph.get(str(article_index), {}).get('out', [])\n",
    "    # 获取相邻文章的LDA向量\n",
    "    neighbor_vectors = [lda_vectors[str(neighbor)] for neighbor in neighbors if str(neighbor) in lda_vectors]\n",
    "\n",
    "    # 如果有相邻文章的LDA向量，则计算平均向量\n",
    "    if neighbor_vectors:\n",
    "        return np.mean(neighbor_vectors, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    lda_vectors_data = load_json('document_topics_vectors.json')\n",
    "    # 将LDA向量数据转换为字典格式，键为文章的Index，值为Vector\n",
    "    lda_vectors = {str(item[\"Index\"]): item[\"Vector\"] for item in lda_vectors_data}\n",
    "    \n",
    "    network_graph = load_json('network_graph.json')\n",
    "\n",
    "    inferred_vectors = {}\n",
    "    # 遍历网络图中的每篇文章\n",
    "    for article_index in network_graph.keys():\n",
    "        if str(article_index) not in lda_vectors:\n",
    "            inferred_vector = infer_topic_vector_for_article(article_index, network_graph, lda_vectors)\n",
    "            if inferred_vector is not None:\n",
    "                inferred_vectors[str(article_index)] = inferred_vector.tolist()\n",
    "\n",
    "    # 将推断出的向量保存到文件中\n",
    "    with open('inferred_LDA_vectors.json', 'w') as outfile:\n",
    "        json.dump(inferred_vectors, outfile, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def convert_to_lda_format(inferred_vectors):\n",
    "    return [{\"Index\": int(index), \"Vector\": vector} for index, vector in inferred_vectors.items()]\n",
    "\n",
    "def main():\n",
    "    lda_vectors = load_json('document_topics_vectors.json') \n",
    "    inferred_vectors = load_json('inferred_LDA_vectors.json') \n",
    "\n",
    "    # 将推断的向量转换为LDA向量格式\n",
    "    inferred_vectors_formatted = convert_to_lda_format(inferred_vectors)\n",
    "\n",
    "    # 将推断的向量合并到LDA向量列表中\n",
    "    merged_vectors = lda_vectors + inferred_vectors_formatted\n",
    "\n",
    "    # 将合并后的向量列表按照Index排序\n",
    "    merged_vectors_sorted = sorted(merged_vectors, key=lambda x: x[\"Index\"])\n",
    "\n",
    "    # 保存合并后的向量到新文件\n",
    "    with open('LDA_vectors_None1.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(merged_vectors_sorted, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def infer_topic_vector_for_article(article_index, network_graph, lda_vectors):\n",
    "    # 获取当前文章的引用和被引用文章索引列表\n",
    "    neighbors = network_graph.get(str(article_index), {}).get('in', []) + network_graph.get(str(article_index), {}).get('out', [])\n",
    "    # 获取相邻文章的LDA向量\n",
    "    neighbor_vectors = [lda_vectors[str(neighbor)] for neighbor in neighbors if str(neighbor) in lda_vectors]\n",
    "\n",
    "    # 如果有相邻文章的LDA向量，则计算平均向量\n",
    "    if neighbor_vectors:\n",
    "        return np.mean(neighbor_vectors, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    lda_vectors_data = load_json('LDA_vectors_None1.json')\n",
    "    # 将LDA向量数据转换为字典格式，键为文章的Index，值为Vector\n",
    "    lda_vectors = {str(item[\"Index\"]): item[\"Vector\"] for item in lda_vectors_data}\n",
    "    \n",
    "    network_graph = load_json('network_graph.json')\n",
    "\n",
    "    inferred_vectors = {}\n",
    "    # 遍历网络图中的每篇文章\n",
    "    for article_index in network_graph.keys():\n",
    "        if str(article_index) not in lda_vectors:\n",
    "            inferred_vector = infer_topic_vector_for_article(article_index, network_graph, lda_vectors)\n",
    "            if inferred_vector is not None:\n",
    "                inferred_vectors[str(article_index)] = inferred_vector.tolist()\n",
    "\n",
    "    # 将推断出的向量保存到文件中\n",
    "    with open('inferred_LDA_vectors_2.json', 'w') as outfile:\n",
    "        json.dump(inferred_vectors, outfile, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def convert_to_lda_format(inferred_vectors):\n",
    "    return [{\"Index\": int(index), \"Vector\": vector} for index, vector in inferred_vectors.items()]\n",
    "\n",
    "def main():\n",
    "    lda_vectors = load_json('LDA_vectors_None1.json') \n",
    "    inferred_vectors = load_json('inferred_LDA_vectors_2.json') \n",
    "\n",
    "    # 将推断的向量转换为LDA向量格式\n",
    "    inferred_vectors_formatted = convert_to_lda_format(inferred_vectors)\n",
    "\n",
    "    # 将推断的向量合并到LDA向量列表中\n",
    "    merged_vectors = lda_vectors + inferred_vectors_formatted\n",
    "\n",
    "    # 将合并后的向量列表按照Index排序\n",
    "    merged_vectors_sorted = sorted(merged_vectors, key=lambda x: x[\"Index\"])\n",
    "\n",
    "    # 保存合并后的向量到新文件\n",
    "    with open('LDA_vectors_None2.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(merged_vectors_sorted, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def infer_topic_vector_for_article(article_index, network_graph, lda_vectors):\n",
    "    # 获取当前文章的引用和被引用文章索引列表\n",
    "    neighbors = network_graph.get(str(article_index), {}).get('in', []) + network_graph.get(str(article_index), {}).get('out', [])\n",
    "    # 获取相邻文章的LDA向量\n",
    "    neighbor_vectors = [lda_vectors[str(neighbor)] for neighbor in neighbors if str(neighbor) in lda_vectors]\n",
    "\n",
    "    # 如果有相邻文章的LDA向量，则计算平均向量\n",
    "    if neighbor_vectors:\n",
    "        return np.mean(neighbor_vectors, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    lda_vectors_data = load_json('LDA_vectors_None2.json')\n",
    "    # 将LDA向量数据转换为字典格式，键为文章的Index，值为Vector\n",
    "    lda_vectors = {str(item[\"Index\"]): item[\"Vector\"] for item in lda_vectors_data}\n",
    "    \n",
    "    network_graph = load_json('network_graph.json')\n",
    "\n",
    "    inferred_vectors = {}\n",
    "    # 遍历网络图中的每篇文章\n",
    "    for article_index in network_graph.keys():\n",
    "        if str(article_index) not in lda_vectors:\n",
    "            inferred_vector = infer_topic_vector_for_article(article_index, network_graph, lda_vectors)\n",
    "            if inferred_vector is not None:\n",
    "                inferred_vectors[str(article_index)] = inferred_vector.tolist()\n",
    "\n",
    "    # 将推断出的向量保存到文件中\n",
    "    with open('inferred_LDA_vectors_3.json', 'w') as outfile:\n",
    "        json.dump(inferred_vectors, outfile, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def convert_to_lda_format(inferred_vectors):\n",
    "    return [{\"Index\": int(index), \"Vector\": vector} for index, vector in inferred_vectors.items()]\n",
    "\n",
    "def main():\n",
    "    lda_vectors = load_json('LDA_vectors_None2.json') \n",
    "    inferred_vectors = load_json('inferred_LDA_vectors_3.json') \n",
    "\n",
    "    # 将推断的向量转换为LDA向量格式\n",
    "    inferred_vectors_formatted = convert_to_lda_format(inferred_vectors)\n",
    "\n",
    "    # 将推断的向量合并到LDA向量列表中\n",
    "    merged_vectors = lda_vectors + inferred_vectors_formatted\n",
    "\n",
    "    # 将合并后的向量列表按照Index排序\n",
    "    merged_vectors_sorted = sorted(merged_vectors, key=lambda x: x[\"Index\"])\n",
    "\n",
    "    # 保存合并后的向量到新文件\n",
    "    with open('LDA_vectors.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(merged_vectors_sorted, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    return cosine_similarity([vector1], [vector2])[0][0]\n",
    "\n",
    "def main():\n",
    "    lda_vectors = load_json('LDA_vectors.json')  # 已合并的LDA向量\n",
    "    # lda_vectors = load_json('document_topics_vectors.json')\n",
    "    network_graph = load_json('merged_network_graph.json')  # 引用网络关系\n",
    "\n",
    "    # 将LDA向量转换为字典形式\n",
    "    lda_dict = {item['Index']: item['Vector'] for item in lda_vectors}\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for relation in network_graph:\n",
    "        cite_id = relation['Cite']\n",
    "        cited_id = relation['Cited']\n",
    "\n",
    "        # 获取引用和被引用文章的主题向量\n",
    "        cite_vector = lda_dict.get(cite_id)\n",
    "        cited_vector = lda_dict.get(cited_id)\n",
    "\n",
    "        if cite_vector is not None and cited_vector is not None:\n",
    "            lda_score = calculate_cosine_similarity(cite_vector, cited_vector)\n",
    "        else:\n",
    "            lda_score = None  # 如果找不到对应的LDA向量，则设置相似度为None\n",
    "\n",
    "        results.append({\n",
    "            \"Cite\": cite_id,\n",
    "            \"Cited\": cited_id,\n",
    "            \"LDA_Score\": lda_score\n",
    "        })\n",
    "\n",
    "    with open('LDA.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(results, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cos方法***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities: 100%|██████████| 26533/26533 [00:09<00:00, 2666.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport json\\nimport numpy as np\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom tqdm import tqdm\\ndef load_json(filename):\\n    with open(filename, \\'r\\') as file:\\n        return json.load(file)\\ndef load_json_lines(filename):\\n    with open(filename, \\'r\\') as file:\\n        return [json.loads(line) for line in file]\\n\\ndef compute_similarity(converted_graph, vectors):\\n    vectors_dict = {item[\"Index\"]: np.array(item[\"Vector\"][0]) for item in vectors}\\n    result = []\\n    for item in tqdm(converted_graph, desc=\"Computing similarities\"):\\n        cite_vector = vectors_dict.get(item[\"Cite\"])\\n        cited_vector = vectors_dict.get(item[\"Cited\"])\\n        if cite_vector is not None and cited_vector is not None:\\n            similarity = cosine_similarity([cite_vector], [cited_vector])[0][0]\\n        else:\\n            similarity = None\\n        item[\"similarity\"] = similarity\\n        result.append(item)\\n    return result\\n\\ndef save_json(data, filename):\\n    with open(filename, \\'w\\', encoding=\\'utf-8\\') as file:\\n        for item in data:\\n            file.write(json.dumps(item) + \"\\n\")\\n\\n# 路径根据实际情况修改\\nconverted_network_graph = load_json(\\'converted_network_graph.json\\')\\noutput_vectors = load_json_lines(\\'output_vectors.json\\')\\n\\n# 计算相似度\\nresult = compute_similarity(converted_network_graph, output_vectors)\\n\\n# 保存结果到新文件\\nsave_json(result, \\'new_file_with_similarities.json\\')\\n\\nprint(\"Completed!\")\\n'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "def load_json_lines(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def compute_similarity(converted_graph, vectors):\n",
    "    vectors_dict = {item[\"Index\"]: np.array(item[\"Vector\"][0]) for item in vectors}\n",
    "    result = []\n",
    "    for item in tqdm(converted_graph, desc=\"Computing similarities\"):\n",
    "        cite_vector = vectors_dict.get(item[\"Cite\"])\n",
    "        cited_vector = vectors_dict.get(item[\"Cited\"])\n",
    "        if cite_vector is not None and cited_vector is not None:\n",
    "            similarity = cosine_similarity([cite_vector], [cited_vector])[0][0]\n",
    "        else:\n",
    "            similarity = None\n",
    "        item[\"similarity\"] = similarity\n",
    "        result.append(item)\n",
    "    return result\n",
    "\n",
    "def save_json(data, filename):\n",
    "    results = []\n",
    "    for item in data:\n",
    "        # 获取引用和被引用的文章ID\n",
    "        cite_id = item[\"Cite\"]\n",
    "        cited_id = item[\"Cited\"]\n",
    "        similarity = item[\"similarity\"]\n",
    "\n",
    "        # 将结果添加到列表中\n",
    "        results.append({\n",
    "            \"Cite\": cite_id,\n",
    "            \"Cited\": cited_id,\n",
    "            \"Cos_Score\": similarity\n",
    "        })\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "\n",
    "converted_network_graph = load_json('merged_network_graph.json')\n",
    "output_vectors = load_json_lines('output_vectors.json')\n",
    "\n",
    "result = compute_similarity(converted_network_graph, output_vectors)\n",
    "\n",
    "save_json(result, 'Cos.json')\n",
    "\n",
    "print(\"Completed!\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return json.load(file)\n",
    "def load_json_lines(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def compute_similarity(converted_graph, vectors):\n",
    "    vectors_dict = {item[\"Index\"]: np.array(item[\"Vector\"][0]) for item in vectors}\n",
    "    result = []\n",
    "    for item in tqdm(converted_graph, desc=\"Computing similarities\"):\n",
    "        cite_vector = vectors_dict.get(item[\"Cite\"])\n",
    "        cited_vector = vectors_dict.get(item[\"Cited\"])\n",
    "        if cite_vector is not None and cited_vector is not None:\n",
    "            similarity = cosine_similarity([cite_vector], [cited_vector])[0][0]\n",
    "        else:\n",
    "            similarity = None\n",
    "        item[\"similarity\"] = similarity\n",
    "        result.append(item)\n",
    "    return result\n",
    "\n",
    "def save_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for item in data:\n",
    "            file.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# 路径根据实际情况修改\n",
    "converted_network_graph = load_json('converted_network_graph.json')\n",
    "output_vectors = load_json_lines('output_vectors.json')\n",
    "\n",
    "# 计算相似度\n",
    "result = compute_similarity(converted_network_graph, output_vectors)\n",
    "\n",
    "# 保存结果到新文件\n",
    "save_json(result, 'new_file_with_similarities.json')\n",
    "\n",
    "print(\"Completed!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***融合***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to Merged_Data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON files\n",
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Merge the JSON data\n",
    "def merge_json_data(lof_data, cos_data, lda_data):\n",
    "    merged_data = []\n",
    "\n",
    "    cos_dict = {(item[\"Cite\"], item[\"Cited\"]): item[\"Cos_Score\"] for item in cos_data}\n",
    "    lda_dict = {(item[\"Cite\"], item[\"Cited\"]): item[\"LDA_Score\"] for item in lda_data}\n",
    "\n",
    "    for index, lof_item in enumerate(lof_data, start=1):\n",
    "        cite = lof_item[\"Cite\"]\n",
    "        cited = lof_item[\"Cited\"]\n",
    "        cos_score = cos_dict.get((cite, cited), None)\n",
    "        lda_score = lda_dict.get((cite, cited), None)\n",
    "\n",
    "        flag = 1 if index > len(lof_data) - 20 else []\n",
    "\n",
    "        merged_item = {\n",
    "            \"index\": index,\n",
    "            \"Cite\": cite,\n",
    "            \"Cited\": cited,\n",
    "            \"LOF_Score\": lof_item[\"LOF_Score\"],\n",
    "            \"Cos_Score\": cos_score,\n",
    "            \"LDA_Score\": lda_score,\n",
    "            \"Score\":[],\n",
    "            \"flag\":flag\n",
    "        }\n",
    "        merged_data.append(merged_item)\n",
    "        \n",
    "    return merged_data\n",
    "\n",
    "lof_json = load_json_file(\"LOF.json\")\n",
    "cos_json = load_json_file(\"Cos.json\")\n",
    "lda_json = load_json_file(\"LDA.json\")\n",
    "merged_json_data = merge_json_data(lof_json, cos_json, lda_json)\n",
    "\n",
    "data = [item for item in merged_json_data if item.get('LDA_Score', 0) < 1]\n",
    "\n",
    "with open(\"Merged_Data.json\", 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"Merged data saved to Merged_Data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***添加flag = 0***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def composite_score(item):\n",
    "    return item.get('LDA_Score', 0) + item.get('Cos_Score', 0) + (1 - item.get('LOF_Score', 2))\n",
    "\n",
    "# 读取JSON文件\n",
    "with open('Merged_Data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "scored_data = [(item['index'], composite_score(item)) for item in data if item.get('flag') != 1]\n",
    "sorted_indices = [index for index, _ in sorted(scored_data, key=lambda x: x[1], reverse=True)[:80]]\n",
    "for item in data:\n",
    "    if item['index'] in sorted_indices:\n",
    "        item['flag'] = 0\n",
    "\n",
    "with open('Merged_Data.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
